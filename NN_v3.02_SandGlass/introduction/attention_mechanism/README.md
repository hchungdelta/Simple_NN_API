# Attention mechanism
Based on paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

Value-key attention model is used in this model. [see algorithm](https://github.com/hchungdelta/Simple_NN_API/tree/master/NN_v3.02_SandGlass/introduction/attention_mechanism/algorithm)
/ [see code](https://github.com/hchungdelta/Simple_NN_API/blob/master/NN_v3.02_SandGlass/ML/Layer/Attention.py)


The original model is as following.
<img src="https://github.com/hchungdelta/Simple_NN_API/blob/master/NN_v3.02_SandGlass/introduction/attention_mechanism/sandglass_base.gif" width="500">

Attention can be added on each layer, in this case, 4 attention layers are added.
<img src="https://github.com/hchungdelta/Simple_NN_API/blob/master/NN_v3.02_SandGlass/introduction/attention_mechanism/sandglass_attn.gif" width="500">



<img src="https://github.com/hchungdelta/Simple_NN_API/blob/master/NN_v3.02_SandGlass/introduction/attention_mechanism/attention_mechanism.gif" width="500">
